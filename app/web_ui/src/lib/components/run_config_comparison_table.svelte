<script lang="ts">
  import type { Eval, Task, TaskRunConfig, EvalResultSummary } from "$lib/types"
  import RunConfigSummary from "$lib/ui/run_config_component/run_config_summary.svelte"
  import OutputTypeTablePreview from "../../routes/(app)/specs/[project_id]/[task_id]/[spec_id]/[eval_id]/output_type_table_preview.svelte"
  import RunEval from "../../routes/(app)/specs/[project_id]/[task_id]/[spec_id]/[eval_id]/run_eval.svelte"
  import { string_to_json_key } from "$lib/utils/json_schema_editor/json_schema_templates"
  import InfoTooltip from "$lib/ui/info_tooltip.svelte"
  import Warning from "$lib/ui/warning.svelte"
  import type { KilnError } from "$lib/utils/error_handlers"

  export let project_id: string
  export let task_id: string
  export let spec_id: string | null = null
  export let eval_id: string
  export let evaluator: Eval
  export let task: Task | null
  export let sorted_task_run_configs: TaskRunConfig[]
  export let score_summary: EvalResultSummary | null
  export let current_eval_config_id: string | null = null
  export let interactive: boolean = false
  export let on_eval_complete: (() => void) | null = null
  export let title: string = "Compare Run Configurations"
  export let subtitle: string =
    "Find the best method of running your task comparing various prompts, models, fine-tunes, and more."
  export let eval_state:
    | "not_started"
    | "running"
    | "complete"
    | "complete_with_errors" = "not_started"
  export let on_add_run_config: (() => void) | null = null
  export let current_eval_config_name: string | null = null
  export let score_summary_error: KilnError | null = null

  function show_incomplete_warning(
    score_summary: EvalResultSummary | null,
  ): boolean {
    if (!score_summary?.run_config_percent_complete) {
      return false
    }

    const values = Object.values(score_summary.run_config_percent_complete)
    const minComplete =
      values.length > 0
        ? values.reduce((min, val) => Math.min(min, val), 1.0)
        : 1.0
    return minComplete < 1.0
  }
</script>

<div class="flex flex-col lg:flex-row gap-4 lg:gap-8 mb-6">
  <div class="grow">
    <div class="text-xl font-bold">{title}</div>
    <div class="text-xs text-gray-500">
      {subtitle}
      {#if interactive && current_eval_config_name}
        <InfoTooltip
          tooltip_text={`Scores are generated by running each 'run config' on each item of your eval dataset, generating task outputs. Then those outputs are evaluated with the selected judge (${current_eval_config_name}).`}
          position="left"
          no_pad={true}
        />
      {/if}
    </div>
  </div>
  <div class="shrink-0">
    <button
      class="btn btn-mid mr-2"
      on:click={() => {
        if (on_add_run_config) {
          on_add_run_config()
        }
      }}>Add Run Configuration</button
    >

    <RunEval
      bind:eval_state
      {project_id}
      {task_id}
      {eval_id}
      {current_eval_config_id}
      run_all={true}
      btn_primary={true}
      eval_type="run_config"
      on_run_complete={() => {
        if (on_eval_complete) {
          on_eval_complete()
        }
      }}
    />
  </div>
</div>

{#if score_summary_error}
  <div class="text-error text-sm mb-4">
    {score_summary_error.getMessage() ||
      "An unknown error occurred fetching scores."}
  </div>
{/if}

<!-- Warn the user if some evals are incomplete -->
{#if show_incomplete_warning(score_summary)}
  <div class="mb-4">
    <button
      class="tooltip tooltip-top cursor-pointer"
      data-tip="Running evals will update any missing dataset items, without re-running complete items. If some evals consistently fail, check the logs for error details."
    >
      <Warning
        warning_message={`Some evals are incomplete and should be excluded from analysis. Click 'Run All Evals' to generate missing results.`}
        tight={true}
      />
    </button>
  </div>
{/if}

<div class="overflow-x-auto rounded-lg border">
  <table class="table table-fixed">
    <thead>
      <tr>
        <th class="max-w-[400px]">
          <div>Run Configuration</div>
          <div class="font-normal">How task output is generated</div>
        </th>
        <th class="text-center">Status</th>
        {#each evaluator.output_scores as output_score}
          <th class="text-center">
            {output_score.name}
            {#if output_score.type}
              <OutputTypeTablePreview output_score_type={output_score.type} />
            {/if}
          </th>
        {/each}
      </tr>
    </thead>
    <tbody>
      {#each sorted_task_run_configs as task_run_config}
        {@const percent_complete =
          score_summary?.run_config_percent_complete?.[
            "" + task_run_config.id
          ] || 0.0}
        <tr class="max-w-[400px]">
          <td>
            <RunConfigSummary
              {task_run_config}
              is_default={task_run_config.id === task?.default_run_config_id}
              {project_id}
              {task_id}
            />
          </td>
          <td class="text-sm text-center">
            {#if percent_complete < 1.0}
              <div class="text-error">
                {(percent_complete * 100.0).toFixed(0)}% Complete
              </div>
              <div class="mt-1">
                {#if interactive && current_eval_config_id}
                  <RunEval
                    {project_id}
                    {task_id}
                    {eval_id}
                    {current_eval_config_id}
                    run_config_ids={[task_run_config.id || ""]}
                    eval_type="run_config"
                    btn_size="xs"
                    btn_primary={false}
                    btn_class="min-w-[120px]"
                    on_run_complete={() => {
                      if (on_eval_complete) {
                        on_eval_complete()
                      }
                    }}
                  />
                {:else}
                  <a
                    href={`/specs/${project_id}/${task_id}/${spec_id}/${eval_id}/compare_run_configs`}
                    class="btn btn-xs btn-outline rounded-full min-w-[120px]"
                  >
                    Run Eval
                  </a>
                {/if}
              </div>
            {:else}
              <div>Complete</div>
            {/if}
            {#if percent_complete > 0}
              <div class="mt-1">
                <a
                  href={`/specs/${project_id}/${task_id}/${spec_id}/${interactive && current_eval_config_id ? current_eval_config_id : evaluator.current_config_id}/${task_run_config.id}/run_result`}
                  class="btn btn-xs btn-outline rounded-full min-w-[120px]"
                >
                  View Data
                </a>
              </div>
            {/if}
          </td>
          {#each evaluator.output_scores as output_score}
            {@const score =
              score_summary?.results?.["" + task_run_config.id]?.[
                string_to_json_key(output_score.name)
              ]?.mean_score}
            <td class="text-center">
              {score != null ? score.toFixed(2) : "unknown"}
            </td>
          {/each}
        </tr>
      {/each}
    </tbody>
  </table>
</div>
